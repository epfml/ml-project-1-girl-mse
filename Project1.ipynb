{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b930523-c9c4-4b69-a5e0-44e3caa05b2b",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "First, we are going to load the data that we'll feed to our alorithm in order for it to learn. We have 3 .csv files to import: <br> *x_train.csv*, *x_test.csv* and *y_train.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21237998-1a1c-4363-bb8d-16b0c387c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = np.genfromtxt(\"x_train.csv\", dtype = float, delimiter=',', skip_header=1)\n",
    "# We have to remove all columns containing nan elements\n",
    "x_train = x_train[:, ~np.isnan(x_train).any(axis=0)]\n",
    "\n",
    "x_test = np.genfromtxt(\"x_test.csv\", dtype=float, delimiter=',', skip_header=1)\n",
    "x_test = x_test[:, ~np.isnan(x_test).any(axis=0)]\n",
    "y_train = np.genfromtxt(\"y_train.csv\", dtype=int, delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "746cd8f3-9c64-41c8-b127-5cca3c1eec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should standardize the data to prevent the gradient from exploding \n",
    "x_train = (x_train - np.mean(x_train, axis=0))/np.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3473698-43b3-40d7-a3ca-6e12afec633f",
   "metadata": {},
   "source": [
    "# Implementing the ML methods\n",
    "\n",
    "## 1.1 Linear regression using gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "034502f1-e50b-4630-ba83-0f9a0382bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to define the function that computes the MSE loss\n",
    "def compute_mse_loss(y, tx, w):\n",
    "    error = y - np.dot(tx, w)\n",
    "    loss = 1/(2*np.shape(error)[0])*np.dot(error.T, error)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Function that computes the gradient\n",
    "def compute_gradient(y, tx, w):\n",
    "    error = y - np.dot(tx, w)\n",
    "    grad = (-1/np.shape(error)[0])*np.dot(tx.T, error)\n",
    "    \n",
    "    return grad   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "704709d3-a323-4770-a51d-82830dc38489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent \n",
    "def mean_squared_error_gd(y, tx, initial_w,  max_iters, gamma):\n",
    "    #Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_mse_loss(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        #Store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws # NOTE: Razlikuje se u tome sto ovo nase treba da vrati samo poslednje vrednosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cceac169-1be6-43a9-ad2e-c37476d664f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.046366715974486265, 0.04368181148825297, 0.042432446271849444, 0.04168683032030336, 0.04118947772886081, 0.0408386452302426, 0.040582363561013426, 0.040390388411048236, 0.04024377689983333, 0.04013006438565057, 0.040040737540258865, 0.03996980639676781, 0.039912952782801894, 0.0398670004547073, 0.03982957357769344, 0.03979886987873761, 0.0397735059688606, 0.03975240941082926, 0.039734741848084815, 0.03971984325463323, 0.039707190851649046, 0.03969636840592636, 0.03968704300614278, 0.039678947310315224, 0.039671865852655364, 0.039665624399756856, 0.039660081622269965, 0.03965512254141479, 0.039650653347011816, 0.03964659728279238, 0.039642891367248585, 0.03963948377199263, 0.03963633171984127, 0.03963339979530774, 0.039630658583446846, 0.039628083570904166, 0.039625654256897916, 0.03962335343267506, 0.039621166596458104, 0.03961908147756956, 0.03961708764868714, 0.03961517620936133, 0.03961333952724815, 0.03961157102615587, 0.03960986501212131, 0.039608216530424435, 0.03960662124780892, 0.039605075355268374, 0.03960357548763684, 0.03960211865693105]\n"
     ]
    }
   ],
   "source": [
    "# We try to train the model - TREBA RAZMISLITI O INICIJALNIM VREDNOSTIMA\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "initial_w = 0.01 * np.random.randn(np.shape(x_train)[1])\n",
    "\n",
    "losses, ws = mean_squared_error_gd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14949940-542f-4a3c-bbf4-a6698cff7e97",
   "metadata": {},
   "source": [
    "## 1.2 Linear regression using stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dbc9770-81b9-47c2-b433-8ed41982954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stochastic gradient we only need one additional function\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    error = y - np.dot(tx, w)\n",
    "    grad = (-1/np.shape(error)[0])*np.dot(tx.T, error)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ccbd9d-8077-41d7-be2e-62f89c383981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using stochastic gradient descent - batch_size = 1\n",
    "def stochastic_gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    #Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # We take one random sample for SGD\n",
    "        index = np.random.randint(np.shape(tx)[0])\n",
    "        x_sample = tx[index, :]\n",
    "        y_sample = y[index]\n",
    "        loss = compute_mse_loss(y, tx, w)\n",
    "        grad = compute_stoch_gradient(y_sample, x_sample, w)\n",
    "        w = w - gamma*grad\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return losses, ws # NOTE: Razlikuje se u tome sto ovo nase treba da vrati samo poslednje vrednosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5730045-62c8-4a66-871d-590e60042495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04970046680477558, 0.04516988650738086, 0.04346491143118687, 0.04248589148259589, 0.04181818158619397, 0.041334357901294916, 0.04097345896285917, 0.040698995113775976, 0.04048713699523808, 0.040321641698876094, 0.04019109884945199, 0.040087290747503994, 0.04000417258150845, 0.03993721832203877, 0.03988298918315943, 0.03983884111787981, 0.03980272142763743, 0.039773023997118864, 0.03974848410500233, 0.0397281006188899, 0.03971107757559965, 0.03969677976723816, 0.03968469862760519, 0.039674425809237926, 0.03966563257586407, 0.03965805363880997, 0.03965147441900715, 0.039645720968644986, 0.03964065197011899, 0.039636152365521586, 0.03963212827137046, 0.03962850291001281, 0.039625213347731225, 0.03962220787465692, 0.039619443896507484, 0.039616886235353005, 0.03961450575788494, 0.03961227826637036, 0.03961018360064399, 0.03960820490990409, 0.03960632806132991, 0.039604541159099726, 0.039602834152607215, 0.03960119851684186, 0.03959962699122612, 0.03959811336586703, 0.03959665230631481, 0.039595239209635806, 0.039593870085982905, 0.03959254146095611]\n"
     ]
    }
   ],
   "source": [
    "# We define all hyperparameters - TREBA DA PRODISKUTUJEMO\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "initial_w = 0.01 * np.random.randn(np.shape(x_train)[1])\n",
    "\n",
    "losses, ws = mean_squared_error_gd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
